<project>
  <title>Real-Time Audio Spectrum Analyzer with AVFoundation and Accelerate in Swift</title>

  <overview>
    We are implementing a real-time spectrum analyzer (bar graph FFT) in an iOS app. The analyzer must visualize both microphone input during live recording and audio file playback.

    The system uses AVFoundation to tap into audio from the mic and from playback, and Accelerate to process FFT and visualize frequency spectrum data. Our goal is low-latency updates to provide a smooth visual experience. 

    The coder agent must construct a Swift-based architecture using `AVAudioEngine` and `vDSP` that:
    - Configures audio input and playback
    - Streams audio buffers
    - Applies FFT to produce frequency-domain data
    - Prepares data for bar graph visualization

    Key considerations include thread safety, reuse of FFT buffers and plans, and latency-sensitive audio pipeline.
  </overview>

  <frameworks>
    <framework name="AVFoundation" purpose="Audio input and playback pipeline" />
    <framework name="Accelerate" purpose="Perform FFT on PCM audio data" />
  </frameworks>

  <interfaces>
    <class name="AudioSpectrumManager">
      <description>Handles all audio engine setup, data taps, FFT processing, and spectrum output updates.</description>
      <properties>
        <property name="fftSize" type="Int" description="Must be power of two. Typically 512, 1024 or 2048." />
        <property name="barCount" type="Int" description="Number of bars in the UI (e.g. 20, 30, 40)." />
        <property name="delegate" type="AudioSpectrumDelegate?" description="Callback for FFT result delivery." />
      </properties>
      <methods>
        <method name="startMicrophoneAnalysis()" description="Begin capturing and analyzing audio from the microphone." />
        <method name="startPlaybackAnalysis(fileURL: URL)" description="Begin analyzing audio from file playback." />
        <method name="stopAnalysis()" description="Stops the audio engine and removes taps." />
      </methods>
    </class>

    <protocol name="AudioSpectrumDelegate">
      <description>Delegate method to receive bar spectrum updates for visualization.</description>
      <methods>
        <method name="func didUpdateSpectrum(_ bars: [Float])" description="Called with scaled frequency magnitudes suitable for bar graph." />
      </methods>
    </protocol>
  </interfaces>

  <contracts>
    <input name="AVAudioPCMBuffer">
      <source>AVAudioEngine input or mixer tap</source>
      <format>PCM Float32 interleaved</format>
      <frameCount>Variable, expected 512–2048</frameCount>
    </input>

    <output name="FFT Result">
      <format>[Float]</format>
      <length>barCount (20–40 values)</length>
      <range>0.0–1.0 normalized magnitudes</range>
    </output>
  </contracts>

  <pseudocode>
    1. Create AudioSpectrumManager class
       - Configure AVAudioEngine
       - Use AVAudioSession with .playAndRecord and .measurement mode
       - Set preferred IO buffer size (~5ms)

    2. Install input tap on inputNode (microphone)
       OR
       Install tap on mainMixerNode for file playback

    3. In tap callback:
       - Access PCM buffer
       - Apply Hann window
       - Perform FFT using vDSP
       - Compute magnitude
       - Normalize & group into barCount bands
       - Send result to delegate on main thread

    4. Provide stopAnalysis() to shut down the engine
  </pseudocode>

  <code>
    <![CDATA[
    import AVFoundation
    import Accelerate

    protocol AudioSpectrumDelegate: AnyObject {
        func didUpdateSpectrum(_ bars: [Float])
    }

    class AudioSpectrumManager {
        private let engine = AVAudioEngine()
        private var fftSetup: FFTSetup?
        private let fftSize: Int
        private let barCount: Int
        private var log2n: vDSP_Length
        weak var delegate: AudioSpectrumDelegate?

        init(fftSize: Int = 1024, barCount: Int = 30) {
            self.fftSize = fftSize
            self.barCount = barCount
            self.log2n = vDSP_Length(log2(Float(fftSize)))
            self.fftSetup = vDSP_create_fftsetup(log2n, FFTRadix(kFFTRadix2))
        }

        func startMicrophoneAnalysis() {
            setupSession()
            let inputNode = engine.inputNode
            let format = inputNode.outputFormat(forBus: 0)

            inputNode.installTap(onBus: 0, bufferSize: AVAudioFrameCount(fftSize), format: format) { [weak self] buffer, _ in
                self?.process(buffer: buffer)
            }

            try? engine.start()
            print("Audio engine started for microphone.")
        }

        func startPlaybackAnalysis(fileURL: URL) {
            let player = AVAudioPlayerNode()
            engine.attach(player)

            guard let file = try? AVAudioFile(forReading: fileURL) else { return }
            let format = file.processingFormat

            engine.connect(player, to: engine.mainMixerNode, format: format)
            engine.mainMixerNode.installTap(onBus: 0, bufferSize: AVAudioFrameCount(fftSize), format: format) { [weak self] buffer, _ in
                self?.process(buffer: buffer)
            }

            try? engine.start()
            player.play()
            player.scheduleFile(file, at: nil)
            print("Playback analysis started.")
        }

        private func setupSession() {
            let session = AVAudioSession.sharedInstance()
            try? session.setCategory(.playAndRecord, mode: .measurement, options: [.defaultToSpeaker])
            try? session.setPreferredIOBufferDuration(0.005)
            try? session.setActive(true)
        }

        private func process(buffer: AVAudioPCMBuffer) {
            guard let fftSetup = fftSetup,
                  let channelData = buffer.floatChannelData?[0] else { return }

            let frameCount = Int(buffer.frameLength)
            let window = vDSP.window(ofType: Float.self,
                                     usingSequence: .hanningDenormalized,
                                     count: frameCount,
                                     isHalfWindow: false)
            var windowed = [Float](repeating: 0, count: frameCount)
            vDSP.multiply(channelData, window, result: &windowed)

            var real = [Float](repeating: 0, count: fftSize/2)
            var imag = [Float](repeating: 0, count: fftSize/2)
            real.withUnsafeMutableBufferPointer { realPtr in
                imag.withUnsafeMutableBufferPointer { imagPtr in
                    var split = DSPSplitComplex(realp: realPtr.baseAddress!, imagp: imagPtr.baseAddress!)
                    windowed.withUnsafeBufferPointer {
                        $0.baseAddress!.withMemoryRebound(to: DSPComplex.self, capacity: fftSize) {
                            vDSP_ctoz($0, 2, &split, 1, vDSP_Length(fftSize / 2))
                        }
                    }

                    vDSP_fft_zrip(fftSetup, &split, 1, log2n, FFTDirection(FFT_FORWARD))

                    var magnitudes = [Float](repeating: 0.0, count: fftSize/2)
                    vDSP_zvabs(&split, 1, &magnitudes, 1, vDSP_Length(fftSize/2))

                    // Convert to decibels or normalize
                    let maxMag = (magnitudes.max() ?? 1.0)
                    if maxMag > 0 {
                        vDSP_vsdiv(magnitudes, 1, [maxMag], &magnitudes, 1, vDSP_Length(magnitudes.count))
                    }

                    let bars = self.reduceToBars(magnitudes: magnitudes)
                    DispatchQueue.main.async {
                        self.delegate?.didUpdateSpectrum(bars)
                    }
                }
            }
        }

        private func reduceToBars(magnitudes: [Float]) -> [Float] {
            let binSize = magnitudes.count / barCount
            var bars = [Float](repeating: 0, count: barCount)

            for i in 0..<barCount {
                let start = i * binSize
                let end = start + binSize
                let slice = magnitudes[start..<end]
                bars[i] = slice.reduce(0, +) / Float(slice.count)
            }

            return bars
        }

        func stopAnalysis() {
            engine.inputNode.removeTap(onBus: 0)
            engine.mainMixerNode.removeTap(onBus: 0)
            engine.stop()
            print("Audio engine stopped.")
        }

        deinit {
            if let fftSetup = fftSetup {
                vDSP_destroy_fftsetup(fftSetup)
            }
        }
    }
    ]]>
  </code>

  <logging>
    - Log when engine starts and stops
    - Log whether we're using mic or playback
    - Log any failure to initialize audio session or read file
    - Optional: log FFT timing to benchmark performance
  </logging>

  <nextSteps>
    - Build a view (e.g. SwiftUI or UIKit) that conforms to `AudioSpectrumDelegate`
    - Implement `didUpdateSpectrum(_:)` to render bars
    - Consider animating bar transitions for smoother display
  </nextSteps>
</project>
